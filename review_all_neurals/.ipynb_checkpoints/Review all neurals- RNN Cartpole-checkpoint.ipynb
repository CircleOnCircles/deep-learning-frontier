{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from neupy import layers, algorithms, environment, storage, plots\n",
    "\n",
    "\n",
    "environment.reproducible()\n",
    "\n",
    "__file__ = os.getcwd() #add to prevent .py: __file__ error\n",
    "\n",
    "CURRENT_DIR = os.path.abspath(os.path.dirname(__file__))\n",
    "FILES_DIR = os.path.join(CURRENT_DIR, 'files')\n",
    "CARTPOLE_WEIGHTS = os.path.join(FILES_DIR, 'cartpole-weights.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_samples(network, memory, gamma=0.9):\n",
    "    data = np.array(memory, dtype=[\n",
    "        ('state', np.ndarray),\n",
    "        ('action', np.int),\n",
    "        ('reward', np.int),\n",
    "        ('done', np.bool),\n",
    "        ('new_state', np.ndarray),\n",
    "    ])\n",
    "\n",
    "    state = np.array(data['state'].tolist())\n",
    "    new_state = np.array(data['new_state'].tolist())\n",
    "\n",
    "    # Note: Calculating Q for all states at once is much faster\n",
    "    # that do it per each sample separately\n",
    "    Q = network.predict(state)\n",
    "    new_Q = network.predict(new_state)\n",
    "    max_Q = np.max(new_Q, axis=1)\n",
    "\n",
    "    n_samples = len(memory)\n",
    "    row_index = np.arange(n_samples)\n",
    "    column_index = data['action']\n",
    "\n",
    "    Q[(row_index, column_index)] = np.where(\n",
    "        data['done'],\n",
    "        data['reward'],\n",
    "        data['reward'] + gamma * max_Q\n",
    "    )\n",
    "\n",
    "    return state, Q\n",
    "\n",
    "\n",
    "def play_game(env, network, n_steps=1000):\n",
    "    state = env.reset()\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        env.render()\n",
    "\n",
    "        q = network.predict(state)\n",
    "        action = int(np.argmax(q[0]))\n",
    "\n",
    "        state, _, done, _ = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "\n",
    "def train_network(env, network, memory, n_games=200, max_score=200,\n",
    "                  epsilon=0.1, gamma=0.9):\n",
    "    for episode in range(1, n_games + 1):\n",
    "        state = env.reset()\n",
    "\n",
    "        for t in range(max_score):\n",
    "            if random.random() <= epsilon:\n",
    "                # Select random action with probability\n",
    "                # equal to the `epsilon`\n",
    "                action = random.randint(0, 1)\n",
    "            else:\n",
    "                # Use action selected by the network\n",
    "                q = network.predict(state)\n",
    "                action = int(np.argmax(q[0]))\n",
    "\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            memory.append((state, action, reward, done, new_state))\n",
    "\n",
    "            if done:\n",
    "                # We done when network lost the game.\n",
    "                # Low reward will penalyze network.\n",
    "                reward = -10\n",
    "\n",
    "            if len(memory) == memory_size:\n",
    "                # Train only when we collected enough samples\n",
    "                x_train, y_train = training_samples(network, memory, gamma)\n",
    "                network.train(x_train, y_train, epochs=1)\n",
    "                loss = network.errors.last()\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if len(memory) == memory_size:\n",
    "            print(\"Game #{:<3} | Lost after {:<3} iterations | loss: {:.4}\"\n",
    "                  \"\".format(episode, t + 1, loss))\n",
    "        else:\n",
    "            print(\"Game #{:<3} | Lost after {:<3} iterations\"\n",
    "                  \"\".format(episode, t + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-28 22:47:00,864] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "network = algorithms.RMSProp(\n",
    "        [\n",
    "            layers.Input(4),\n",
    "\n",
    "            layers.Relu(64),\n",
    "            layers.Relu(48),\n",
    "            layers.Relu(32),\n",
    "            layers.Relu(64) > layers.Dropout(0.2),\n",
    "\n",
    "            # Expecting two different actions:\n",
    "            # 1. Move left\n",
    "            # 2. Move right\n",
    "            layers.Linear(2),\n",
    "        ],\n",
    "\n",
    "        step=0.001,\n",
    "        error='rmse',\n",
    "        batch_size=100,\n",
    "\n",
    "        decay_rate=0.1,\n",
    "        addons=[algorithms.WeightDecay],\n",
    "    )\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)  # To make results reproducible for the gym\n",
    "\n",
    "memory_size = 1000  # Number of samples stored in the memory\n",
    "memory = deque(maxlen=memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network.architecture()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#dont forget\n",
    "brew install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plots.layer_structure(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'ndim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/Users/heim/Dropbox/DataShop/Review all neurals\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaliency_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cat.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/heim/anaconda/lib/python3.5/site-packages/neupy/plots/saliency_map.py\u001b[0m in \u001b[0;36msaliency_map\u001b[0;34m(connection, image, mode, sigma, ax, show, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mplots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaliency_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdog_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'ndim'"
     ]
    }
   ],
   "source": [
    "plots.saliency_map(network,'cat.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Loading pretrained weights\")\n",
    "storage.load(network, CARTPOLE_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Start training\")\n",
    "train_network(\n",
    "    env, network, memory,\n",
    "    n_games=120,  # Number of games that networks is going to play,\n",
    "    max_score=200,  # Maximum score that network can achive in the game\n",
    "    epsilon=0.1,  # Probability to select random action during the game\n",
    "    gamma=0.99)\n",
    "\n",
    "if not os.path.exists(FILES_DIR):\n",
    "    os.mkdir(FILES_DIR)\n",
    "\n",
    "print(\"Saving parameters\")\n",
    "storage.save(network, CARTPOLE_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Start playing game\")\n",
    "play_game(env, network, n_steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
